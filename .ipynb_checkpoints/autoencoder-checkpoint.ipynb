{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ff721e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tifffile as tiff\n",
    "from glob import glob\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage import io\n",
    "import cv2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU...\")\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    print(\"Using CPU...\")\n",
    "    dev = \"cpu\"\n",
    "    \n",
    "    \n",
    "device = torch.device(dev)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Set the random seed for reproducibility \n",
    "torch.manual_seed(2020) \n",
    "\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Transform the 28 by 28 image to an embedded code size of 30\n",
    "            nn.Linear(38400, 30),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(30, 38400),\n",
    "            nn.Sigmoid()  #to range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x will be a (batch_size,1,28,28) tensor when using MNIST\n",
    "        # so we will reshape it to a (batch_size, 28*28) flat tensor.\n",
    "        # After it has been decoded we will reshape back to the image shape.\n",
    "        x = self.encoder(x.view(-1, 38400))\n",
    "        x = self.decoder(x)\n",
    "        return x.view(-1, 1, 160, 240)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 1 input image channel, 16 output channel, 3x3 square convolution\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),  # activation function\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # conv layer\n",
    "            nn.ReLU(), # activation function \n",
    "            nn.Conv2d(32, 64, kernel_size=7, stride=1, padding=0) # conv layer\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=7, stride=1, padding=0), # conv transpose layer\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # conv transpose layer\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  #to range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ConvAutoencoder2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder2D, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 1 input image channel, 16 output channel, 3x3 square convolution\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),  # activation function\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # conv layer\n",
    "            nn.ReLU(), # activation function \n",
    "            nn.Conv2d(32, 64, kernel_size=7, stride=1, padding=0), # conv layer\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear( 64*1*1, 2)\n",
    "            nn.Linear( 64*1*1, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=7, stride=1, padding=0), # conv transpose layer\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # conv transpose layer\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  #to range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f1c0287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test_all\\Test001_001.tif\n"
     ]
    }
   ],
   "source": [
    "imgA = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test_all/Test017_141.tif'\n",
    "imgB = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test_all/Test017_143.tif'\n",
    "\n",
    "\n",
    "imlist = [imgA,imgB]\n",
    "\n",
    "w,h=Image.open(imlist[0]).size\n",
    "N=len(imlist)\n",
    "\n",
    "arr=np.zeros((h,w),float)\n",
    "\n",
    "\n",
    "for im in imlist:\n",
    "    imarr=np.array(Image.open(im),dtype=float)\n",
    "    arr=arr+imarr/N\n",
    "\n",
    "\n",
    "arr=np.array(np.round(arr),dtype=np.uint8)\n",
    "\n",
    "out=Image.fromarray(arr)\n",
    "#out.save(\"Test017_142.tif\")\n",
    "#out.show()\n",
    "\n",
    "\n",
    "path = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test_all'\n",
    "imageList = [os.path.join(path, file) for file in os.listdir(path)]\n",
    "print(imageList[0])\n",
    "\n",
    "w,h=Image.open(imageList[0]).size\n",
    "N=len(imageList)\n",
    "\n",
    "avgArr=np.zeros((h,w),float)\n",
    "\n",
    "for im in imageList:\n",
    "    imageArr=np.array(Image.open(im),dtype=float)\n",
    "    avgArr=avgArr+imageArr/N\n",
    "    \n",
    "avgArr=np.array(np.round(avgArr),dtype=np.uint8)\n",
    "    \n",
    "avg=Image.fromarray(avgArr)\n",
    "#avg.save(\"Test_avg.tif\")\n",
    "avg.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "292923ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "class UCSDAnomalyDataset(data.Dataset):\n",
    "    '''\n",
    "    Dataset class to load  UCSD Anomaly Detection dataset\n",
    "    Input: \n",
    "    - root_dir -- directory (Train/Test) structured exactly as out-of-the-box folder downloaded from the site\n",
    "    http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm\n",
    "    - time_stride (default 1) -- max possible time stride used for data augmentation\n",
    "    - seq_len -- length of the frame sequence\n",
    "    Output:\n",
    "    - tensor of 10 normlized grayscale frames stiched together\n",
    "    \n",
    "    Note:\n",
    "    [mean, std] for grayscale pixels is [0.3750352255196134, 0.20129592430286292]\n",
    "    '''\n",
    "    def __init__(self, root_dir, seq_len = 1, time_stride=1, transform=None):\n",
    "        super(UCSDAnomalyDataset, self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        vids = [d for d in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, d))]\n",
    "        self.samples = []\n",
    "        for d in vids:\n",
    "            for t in range(1, time_stride+1):\n",
    "                for i in range(1, 200):\n",
    "                    if i+(seq_len-1)*t > 200:\n",
    "                        break\n",
    "                    self.samples.append((os.path.join(self.root_dir, d), range(i, i+(seq_len-1)*t+1, t)))\n",
    "        self.pil_transform = transforms.Compose([\n",
    "                    #transforms.Resize((227, 227)),\n",
    "                    transforms.Grayscale(),\n",
    "                    transforms.ToTensor()])\n",
    "        #self.tensor_transform = transforms.Compose([\n",
    "        #            transforms.Normalize(mean=(0.3750352255196134,), std=(0.20129592430286292,))])\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sample = []\n",
    "        pref = self.samples[index][0]\n",
    "        for fr in self.samples[index][1]:\n",
    "            with open(os.path.join(pref, '{0:03d}.tif'.format(fr)), 'rb') as fin:\n",
    "                frame = Image.open(fin).convert('RGB')\n",
    "                frame = self.pil_transform(frame) / 255.0\n",
    "                #print(frame.shape)\n",
    "                #frame = self.tensor_transform(frame)\n",
    "                sample.append(frame)\n",
    "        sample = torch.stack(sample, axis=0)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AnomalyDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_dir)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_dir[index]\n",
    "        image = Image.open(img_path)\n",
    "        #avg = Image.open('Test_avg.tif')\n",
    "        #image1 = np.asarray(image)\n",
    "        #image2 = np.asarray(avg)\n",
    "        #image = image1 - image2\n",
    "        #image = Image.fromarray(image)\n",
    "        #image.show()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "      \n",
    "      \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "image_path_train = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train_all'\n",
    "image_paths_train = glob(image_path_train + '/*.tif')\n",
    "data_transforms = transforms.Compose([transforms.Resize((160, 240)), transforms.ToTensor()])\n",
    "\n",
    "#dataset = VideoFrameDataset(\n",
    "#    root_path='./UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train',\n",
    "#    num_segments=5,\n",
    "#    frames_per_segment=1,\n",
    "#    imagefile_template='{:03d}.tif',\n",
    "#    transform=None,\n",
    "#    test_mode=False\n",
    "#)\n",
    "\n",
    "\n",
    "image_path_test = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test_all'\n",
    "image_paths_test = glob(image_path_test + '/*.tif')\n",
    "\n",
    "dataset_train = AnomalyDataset(image_paths_train, data_transforms)\n",
    "dataset_train2 = AnomalyDataset(image_paths_test, data_transforms)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#dataset = UCSDAnomalyDataset('./UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train', time_stride=1)\n",
    "#data_train = data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c65e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AE(model, dataset, max_epochs=20, print_steps=5):\n",
    "    #Training (optimisation) parameters\n",
    "    batch_size=64\n",
    "    learning_rate=1e-3\n",
    "\n",
    "    #Choose mean square error loss\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "    #Choose the Adam optimiser\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    #Specify how the data will be loaded in batches (with random shuffling)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    #Storage\n",
    "    outputs = []\n",
    "    \n",
    "    model.to(device)\n",
    "    #train_loader = train_loader.cuda()\n",
    "\n",
    "    #Start training\n",
    "    for epoch in range(max_epochs):\n",
    "        for img in train_loader:\n",
    "            img = img.to(device)\n",
    "            recon = model(img)\n",
    "            #print(recon.shape)\n",
    "            loss = criterion(recon, img.to(device))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "        #if ((epoch % print_steps) == 0) or (epoch +1 == max_epochs):\n",
    "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, loss.item()))\n",
    "        outputs.append((epoch, img.detach(), recon.detach()),)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd7a1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#cae.to(device)\n",
    "\n",
    "def do_pca(batch_size, n_components, dataset):\n",
    "    \n",
    "    pca = PCA(n_components)\n",
    "\n",
    "    # Create a dataloader to pass all the training data\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    images = next(iter(train_loader))\n",
    "\n",
    "    transformed_images = pca.fit_transform(images.reshape(-1, 160*240))\n",
    "    reconstructed_images = pca.inverse_transform(transformed_images)\n",
    "    \n",
    "\n",
    "    #print(np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "    plt.plot(np.arange(1,n_components+1), pca.explained_variance_ratio_)\n",
    "    # Plotting using a log scale might show more information\n",
    "    #plt.yscale('log')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.arange(1,n_components+1), np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    reconstruction_error = mean_squared_error(images.numpy().reshape(-1, 160*240), reconstructed_images)\n",
    "    print('Reconstruction error is ', reconstruction_error)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for i in range(8):\n",
    "        # Top row: show original faces\n",
    "        plt.subplot(2,8,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(images[i].reshape(160,240), cmap='Greys_r')\n",
    "        # Bottom row: show reconstructions\n",
    "        plt.subplot(2,8, 8+i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(reconstructed_images[i].reshape(160,240), cmap='Greys_r')\n",
    "    plt.show()\n",
    "    \n",
    "    return transformed_images\n",
    "\n",
    "    \n",
    "    \n",
    "#print(\"==========PCA TRAIN==========\")\n",
    "#do_pca(6800, 200, dataset_train)\n",
    "#do_pca(6800, 400, dataset_train)\n",
    "#do_pca(6800, 600, dataset_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"==========PCA TEST==========\")\n",
    "pca_200 = do_pca(7200, 200, dataset_train2)\n",
    "pca_400 = do_pca(7200, 400, dataset_train2)\n",
    "pca_600 = do_pca(7200, 600, dataset_train2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1bb91c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ae = Autoencoder()\n",
    "cae = ConvAutoencoder()\n",
    "cae2D = ConvAutoencoder2D()\n",
    "\n",
    "#print(ae)\n",
    "#print(\"==============Standard Autoencoder===========\")\n",
    "#outputs = train_AE(ae, dataset, max_epochs=30, print_steps=30)\n",
    "#print(\"\\n\\n\")\n",
    "\n",
    "\"\"\"\n",
    "numImgs=12;\n",
    "for k in range(0, len(outputs), 9):\n",
    "    plt.figure(figsize=(numImgs, 2))\n",
    "    imgs = outputs[k][1].numpy()    \n",
    "    recon = outputs[k][2].numpy()\n",
    "    print('Epoch:', k+1)\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= numImgs: break\n",
    "        plt.subplot(2, numImgs, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(item[0])\n",
    "        \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= numImgs: break\n",
    "        plt.subplot(2, numImgs, numImgs+i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(item[0])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#print(\"==============ConvAutoencoder TRAIN==============\")\n",
    "#outputs = train_AE(cae, dataset_train, max_epochs=30, print_steps=30)\n",
    "#print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "print(\"\\n\\n\")\n",
    "print(\"==============ConvAutoencoder TEST==============\")\n",
    "#outputs = train_AE(cae, dataset_train2, max_epochs=30, print_steps=30)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "numOut = len(outputs)-1\n",
    "numImgs=12;\n",
    "for k in range(0, len(outputs), 9):\n",
    "    plt.figure(figsize=(numImgs, 2))\n",
    "    imgs = outputs[k][1].cpu().numpy()    \n",
    "    recon = outputs[k][2].cpu().numpy()\n",
    "    print('Epoch:', k+1)\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= numImgs: break\n",
    "        plt.subplot(2, numImgs, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(item[0])\n",
    "        \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= numImgs: break\n",
    "        plt.subplot(2, numImgs, numImgs+i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(item[0])\n",
    "        \n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "imgs = outputs[numOut][1].cpu().numpy()    \n",
    "recon = outputs[numOut][2].cpu().numpy()\n",
    "\n",
    "for i in range(8):\n",
    "    # Top row: show original faces\n",
    "    plt.subplot(2,8,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(imgs[i][0].reshape(160,240), cmap='Greys_r')\n",
    "    # Bottom row: show reconstructions\n",
    "    plt.subplot(2,8, 8+i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(recon[i][0].reshape(160,240), cmap='Greys_r')\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145e2e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#for i in range (2,37):\n",
    "#    print(i, \" :\", i*200)\n",
    "\n",
    "    \n",
    "labels = np.zeros(7200)\n",
    "import csv\n",
    "with open('labels.txt', 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if(\",\" in line):\n",
    "            #part1 = line.rstrip().split(',')[0]\n",
    "            #part2 = line.rstrip().split(',')[1]\n",
    "            start1 = int(line.rstrip().split(',')[0].split(':')[0])-1+(i*200)\n",
    "            end1 = int(line.rstrip().split(',')[0].split(':')[1])+1+(i*200)\n",
    "            \n",
    "            labels[start1:end1] = 1\n",
    "            \n",
    "            start2 = int(line.rstrip().split(',')[1].split(':')[0])-1+(i*200)\n",
    "            end2 = int(line.rstrip().split(',')[1].split(':')[1].replace(';', ''))+1+(i*200)\n",
    "            \n",
    "            labels[start2:end2] = 1\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            start3 = int(line.rstrip().split(';')[0].split(':')[0])-1+(i*200)\n",
    "            end3 = int(line.rstrip().split(';')[0].split(':')[1])+1+(i*200)\n",
    "            #s = slice(*map(int, frames.split(':')))\n",
    "            #print(s)\n",
    "            labels[start3:end3] = 1\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e82334",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self): \n",
    "        super(LinearRegression, self).__init__() \n",
    "        self.linear = torch.nn.Linear(38400, 1)\n",
    "\n",
    "      \n",
    "    def forward(self, x): \n",
    "\n",
    "        predict_y = self.linear(x) \n",
    "        return predict_y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f207e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLinearModel(model, dataset, labels, max_epochs=20, print_steps=5):\n",
    "    #Training (optimisation) parameters\n",
    "    learning_rate=1e-3\n",
    "\n",
    "    #Choose mean square error loss\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "    #Choose the Adam optimiser\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    images = torch.from_numpy(dataset)\n",
    "\n",
    "\n",
    "    #Storage\n",
    "    outputs = []\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    #Start training\n",
    "    for epoch in range(max_epochs):\n",
    "        for img, labels in images:\n",
    "            img = img.to(device)\n",
    "            predict = model(img.float())\n",
    "            #print(recon.shape)\n",
    "            loss = criterion(predict, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "        #if ((epoch % print_steps) == 0) or (epoch +1 == max_epochs):\n",
    "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, loss.item()))\n",
    "        outputs.append((epoch, img.detach(), recon.detach()),)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45544f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linearModel = LinearRegression()\n",
    "\n",
    "#linearOutputs = trainLinearModel(linearModel, pca_400, labels, 50, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30abfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
