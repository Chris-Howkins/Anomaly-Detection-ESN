{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff721e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tifffile as tiff\n",
    "from glob import glob\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage import io\n",
    "import cv2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, f1_score, balanced_accuracy_score, precision_score, recall_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils import compute_sample_weight\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU...\")\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    print(\"Using CPU...\")\n",
    "    dev = \"cpu\"\n",
    "    \n",
    "    \n",
    "device = torch.device(dev)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Set the random seed for reproducibility \n",
    "torch.manual_seed(2020) \n",
    "\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Transform the 28 by 28 image to an embedded code size of 30\n",
    "            nn.Linear(38400, 30),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(30, 38400),\n",
    "            nn.Sigmoid()  #to range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x will be a (batch_size,1,28,28) tensor when using MNIST\n",
    "        # so we will reshape it to a (batch_size, 28*28) flat tensor.\n",
    "        # After it has been decoded we will reshape back to the image shape.\n",
    "        x = self.encoder(x.view(-1, 38400))\n",
    "        x = self.decoder(x)\n",
    "        return x.view(-1, 1, 160, 240)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 1 input image channel, 16 output channel, 3x3 square convolution\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),  # activation function\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # conv layer\n",
    "            nn.ReLU(), # activation function \n",
    "            nn.Conv2d(32, 64, kernel_size=7, stride=1, padding=0) # conv layer\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=7, stride=1, padding=0), # conv transpose layer\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # conv transpose layer\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  #to range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ConvAutoencoder2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder2D, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 1 input image channel, 16 output channel, 3x3 square convolution\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),  # activation function\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # conv layer\n",
    "            nn.ReLU(), # activation function \n",
    "            nn.Conv2d(32, 64, kernel_size=7, stride=1, padding=0), # conv layer\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear( 64*1*1, 2)\n",
    "            nn.Linear( 64*1*1, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=7, stride=1, padding=0), # conv transpose layer\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # conv transpose layer\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  #to range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgA = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test_all/Test017_141.tif'\n",
    "imgB = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test_all/Test017_143.tif'\n",
    "\n",
    "\n",
    "imlist = [imgA,imgB]\n",
    "\n",
    "w,h=Image.open(imlist[0]).size\n",
    "N=len(imlist)\n",
    "\n",
    "arr=np.zeros((h,w),float)\n",
    "\n",
    "\n",
    "for im in imlist:\n",
    "    imarr=np.array(Image.open(im),dtype=float)\n",
    "    arr=arr+imarr/N\n",
    "\n",
    "\n",
    "arr=np.array(np.round(arr),dtype=np.uint8)\n",
    "\n",
    "out=Image.fromarray(arr)\n",
    "#out.save(\"017_142.tif\")\n",
    "#out.show()\n",
    "\n",
    "\n",
    "path = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test_all'\n",
    "imageList = [os.path.join(path, file) for file in os.listdir(path)]\n",
    "print(imageList[0])\n",
    "\n",
    "w,h=Image.open(imageList[0]).size\n",
    "N=len(imageList)\n",
    "\n",
    "avgArr=np.zeros((h,w),float)\n",
    "\n",
    "for im in imageList:\n",
    "    imageArr=np.array(Image.open(im),dtype=float)\n",
    "    avgArr=avgArr+imageArr/N\n",
    "    \n",
    "avgArr=np.array(np.round(avgArr),dtype=np.uint8)\n",
    "    \n",
    "avg=Image.fromarray(avgArr)\n",
    "#avg.save(\"Test_avg.tif\")\n",
    "avg.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292923ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "\"\"\"\n",
    "class AnomalyDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.folder_names = os.listdir(self.img_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.folder_names)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #img_path = self.img_dir[index]\n",
    "        #image = Image.open(img_path)\n",
    "        #avg = Image.open('Test_avg.tif')\n",
    "        #image1 = np.asarray(image)\n",
    "        #image2 = np.asarray(avg)\n",
    "        #image = image1 - image2\n",
    "        #image = Image.fromarray(image)\n",
    "        #image.show()\n",
    "        #if self.transform:\n",
    "        #    image = self.transform(image)\n",
    "        \n",
    "        folder_path = os.path.join(self.img_dir, self.folder_names[index])\n",
    "        image_filenames = os.listdir(folder_path)\n",
    "        \n",
    "        images = []\n",
    "        for img in image_filenames:\n",
    "            if img.endswith('.tif'):\n",
    "                image_path = os.path.join(folder_path, img)\n",
    "                image = Image.open(image_path)\n",
    "            \n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                images.append(image)\n",
    "            \n",
    "        return images\n",
    "      \n",
    "      \n",
    "\"\"\"   \n",
    "\n",
    "        \n",
    "path = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/All_data'\n",
    "data_transforms = transforms.Compose([transforms.Resize((160, 240)), transforms.ToTensor()])\n",
    "\n",
    "#dataset = AnomalyDataset(path, data_transforms)\n",
    "\n",
    "\n",
    "\n",
    "#dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "root_dir = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/All_data'\n",
    "num_folders = 70\n",
    "num_images_per_folder = 200\n",
    "image_height = 160\n",
    "image_width = 240\n",
    "\n",
    "dataset = np.zeros((num_folders, num_images_per_folder, image_height, image_width))\n",
    "\n",
    "for i in range(num_folders):\n",
    "    folder_name = str(i+1).zfill(3)\n",
    "    folder_path = os.path.join(root_dir, folder_name)\n",
    "\n",
    "    for j in range(num_images_per_folder):\n",
    "        image_name = str(j+1).zfill(3) + '.tif'\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((240, 160))\n",
    "        \n",
    "        image = np.array(image)\n",
    "        dataset[i, j, :, :] = image\n",
    "    \n",
    "    \n",
    "    \n",
    "print(dataset.shape)\n",
    "dataset_norm = dataset/255.0\n",
    "\n",
    "np.save('UCSD_Ped1', dataset_norm)\n",
    "\n",
    "#dataset_tensor = torch.tensor(dataset)\n",
    "#print(dataset_tensor.shape)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "#image_path_train = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train_all'\n",
    "#image_paths_train = glob(image_path_train + '/*.tif')\n",
    "#data_transforms = transforms.Compose([transforms.Resize((160, 240)), transforms.ToTensor()])\n",
    "\n",
    "\n",
    "\n",
    "#image_path_test = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test_all'\n",
    "#image_paths_test = glob(image_path_test + '/*.tif')\n",
    "\n",
    "#dataset_train = AnomalyDataset(image_paths_train, data_transforms)\n",
    "#dataset_train2 = AnomalyDataset(image_paths_test, data_transforms)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#dataset = UCSDAnomalyDataset('./UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train', time_stride=1)\n",
    "#data_train = data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ede3b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_start = 34\n",
    "labels = np.zeros((num_folders, num_images_per_folder, 1))\n",
    "\"\"\"    \n",
    "labels = np.zeros(7200, dtype=np.int8)\n",
    "import csv\n",
    "with open('labels.txt', 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if(\",\" in line):\n",
    "            #part1 = line.rstrip().split(',')[0]\n",
    "            #part2 = line.rstrip().split(',')[1]\n",
    "            start1 = int(line.rstrip().split(',')[0].split(':')[0])-1+(i*200)\n",
    "            end1 = int(line.rstrip().split(',')[0].split(':')[1])+1+(i*200)\n",
    "            \n",
    "            labels[start1:end1] = 1\n",
    "            \n",
    "            start2 = int(line.rstrip().split(',')[1].split(':')[0])-1+(i*200)\n",
    "            end2 = int(line.rstrip().split(',')[1].split(':')[1].replace(';', ''))+1+(i*200)\n",
    "            \n",
    "            labels[start2:end2] = 1\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            start3 = int(line.rstrip().split(';')[0].split(':')[0])-1+(i*200)\n",
    "            end3 = int(line.rstrip().split(';')[0].split(':')[1])+1+(i*200)\n",
    "            #s = slice(*map(int, frames.split(':')))\n",
    "            #print(s)\n",
    "            labels[start3:end3] = 1\n",
    "            \n",
    "\"\"\"\n",
    "            \n",
    "            \n",
    "            \n",
    "with open(\"labels.txt\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        parts = line.strip().split(\";\")\n",
    "        if (\",\" in line):\n",
    "            indices = parts[0].strip().split(\",\")\n",
    "            for index in indices:\n",
    "                start, end = index.strip().split(\":\")\n",
    "                start = int(start) - 1\n",
    "                end = int(end) - 1\n",
    "                \n",
    "                labels[i+label_start, start:end, 0] = 1\n",
    "        else:\n",
    "            start, end = parts[0].strip().split(\":\")\n",
    "            start = int(start) - 1\n",
    "            end = int(end) - 1\n",
    "            \n",
    "            labels[i+label_start, start:end, 0] = 1\n",
    "                \n",
    "                \n",
    "                \n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d5d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd7a1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_norm, labels, test_size=0.2, random_state=42)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "#img = Image.fromarray(np.uint8(X_train[0][0]))\n",
    "\n",
    "#plt.imshow(img)\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cae.to(device)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def do_pca(n_components, train_data, test_data):\n",
    "    pca = PCA(n_components)\n",
    "    \n",
    "    #train_data = train_data.numpy()\n",
    "    num_videos, num_frames, height, width = train_data.shape\n",
    "    data_train = train_data.reshape(num_videos*num_frames, height*width)\n",
    "    print(data_train.shape)\n",
    "    \n",
    "    transformed_train_images = pca.fit_transform(data_train)\n",
    "    reconstructed_train_images = pca.inverse_transform(transformed_train_images)\n",
    "    print(reconstructed_train_images.shape)\n",
    "    \n",
    "    #test_data = test_data.numpy()\n",
    "    num_videos, num_frames, height, width = test_data.shape   \n",
    "    data_test = test_data.reshape(num_videos*num_frames, height*width)\n",
    "    transformed_test_images = pca.transform(data_test)\n",
    "    reconstructed_test_images = pca.inverse_transform(transformed_test_images)\n",
    "    \n",
    "\n",
    "    #print(np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "    plt.plot(np.arange(1,n_components+1), pca.explained_variance_ratio_)\n",
    "    # Plotting using a log scale might show more information\n",
    "    #plt.yscale('log')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.arange(1,n_components+1), np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    reconstruction_error_train = mean_squared_error(data_train, reconstructed_train_images)\n",
    "    print('Train Reconstruction error is ', reconstruction_error_train)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for i in range(8):\n",
    "        # Top row: show original faces\n",
    "        plt.subplot(2,8,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(data_train[i].reshape(160,240), cmap='Greys_r')\n",
    "        # Bottom row: show reconstructions\n",
    "        plt.subplot(2,8, 8+i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(reconstructed_train_images[i].reshape(160,240), cmap='Greys_r')\n",
    "    plt.show()\n",
    "    \n",
    "    reconstruction_error_test = mean_squared_error(data_test, reconstructed_test_images)\n",
    "    print('Test Reconstruction error is ', reconstruction_error_test)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for i in range(8):\n",
    "        # Top row: show original faces\n",
    "        plt.subplot(2,8,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(data_test[i].reshape(160,240), cmap='Greys_r')\n",
    "        # Bottom row: show reconstructions\n",
    "        plt.subplot(2,8, 8+i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(reconstructed_test_images[i].reshape(160,240), cmap='Greys_r')\n",
    "    plt.show()\n",
    "    \n",
    "    return transformed_train_images, transformed_test_images\n",
    "\n",
    "    \n",
    "    \n",
    "print(\"==========PCA TRAIN/TEST==========\")\n",
    "#pca_100_train, pca_100_test = do_pca(100, X_train, X_test)\n",
    "#pca_200_train, pca_200_test = do_pca(200, X_train, X_test)\n",
    "#pca_400_train, pca_400_test = do_pca(400, X_train, X_test)\n",
    "#pca_600_train, pca_600_test = do_pca(600, X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1bb91c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ae = Autoencoder()\n",
    "cae = ConvAutoencoder()\n",
    "cae2D = ConvAutoencoder2D()\n",
    "\n",
    "#print(ae)\n",
    "#print(\"==============Standard Autoencoder===========\")\n",
    "#outputs = train_AE(ae, dataset, max_epochs=30, print_steps=30)\n",
    "#print(\"\\n\\n\")\n",
    "\n",
    "\"\"\"\n",
    "numImgs=12;\n",
    "for k in range(0, len(outputs), 9):\n",
    "    plt.figure(figsize=(numImgs, 2))\n",
    "    imgs = outputs[k][1].numpy()    \n",
    "    recon = outputs[k][2].numpy()\n",
    "    print('Epoch:', k+1)\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= numImgs: break\n",
    "        plt.subplot(2, numImgs, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(item[0])\n",
    "        \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= numImgs: break\n",
    "        plt.subplot(2, numImgs, numImgs+i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(item[0])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"==============ConvAutoencoder TRAIN==============\")\n",
    "outputs = train_AE(cae, X_train, max_epochs=30, print_steps=30)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "print(\"\\n\\n\")\n",
    "print(\"==============ConvAutoencoder TEST==============\")\n",
    "outputs = train_AE(cae, X_test, max_epochs=30, print_steps=30)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "numOut = len(outputs)-1\n",
    "numImgs=12;\n",
    "for k in range(0, len(outputs), 9):\n",
    "    plt.figure(figsize=(numImgs, 2))\n",
    "    imgs = outputs[k][1].cpu().numpy()    \n",
    "    recon = outputs[k][2].cpu().numpy()\n",
    "    print('Epoch:', k+1)\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= numImgs: break\n",
    "        plt.subplot(2, numImgs, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(item[0])\n",
    "        \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= numImgs: break\n",
    "        plt.subplot(2, numImgs, numImgs+i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(item[0])\n",
    "        \n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "imgs = outputs[numOut][1].cpu().numpy()    \n",
    "recon = outputs[numOut][2].cpu().numpy()\n",
    "\n",
    "for i in range(8):\n",
    "    # Top row: show original faces\n",
    "    plt.subplot(2,8,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(imgs[i][0].reshape(160,240), cmap='Greys_r')\n",
    "    # Bottom row: show reconstructions\n",
    "    plt.subplot(2,8, 8+i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(recon[i][0].reshape(160,240), cmap='Greys_r')\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55fc995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e82334",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AE(model, dataset, max_epochs=20, print_steps=5):\n",
    "    #Training (optimisation) parameters\n",
    "    batch_size=64\n",
    "    learning_rate=1e-3\n",
    "\n",
    "    #Choose mean square error loss\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "    #Choose the Adam optimiser\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    #Specify how the data will be loaded in batches (with random shuffling)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    #Storage\n",
    "    outputs = []\n",
    "    \n",
    "    model.to(device)\n",
    "    #train_loader = train_loader.cuda()\n",
    "\n",
    "    #Start training\n",
    "    for epoch in range(max_epochs):\n",
    "        for img in train_loader:\n",
    "            img = img.to(device)\n",
    "            recon = model(img.float())\n",
    "            #print(recon.shape)\n",
    "            loss = criterion(recon, img.to(device))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "        #if ((epoch % print_steps) == 0) or (epoch +1 == max_epochs):\n",
    "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, loss.item()))\n",
    "        outputs.append((epoch, img.detach(), recon.detach()),)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f207e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba1eeae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(outputs[29][1].cpu().numpy().shape)\n",
    "#print(outputs[29][])\n",
    "#print(pca_400.shape)\n",
    "\n",
    "#linearModel = LinearRegression()\n",
    "\n",
    "#linearOutputs = trainLinearModel(linearModel, pca_400, labels, 50, 50)\n",
    "\n",
    "V, F = y_train.shape[:2]\n",
    "labels_train = np.reshape(y_train, (V*F, 1))\n",
    "\n",
    "V, F = y_test.shape[:2]\n",
    "labels_test = np.reshape(y_test, (V*F, 1))\n",
    "labels_test_log = labels_test.ravel()\n",
    "labels_train_log = labels_train.ravel()\n",
    "\n",
    "linearPCA = RidgeClassifier().fit(pca_400_train, labels_train_log)\n",
    "\n",
    "y_pred = linearPCA.predict(pca_400_test)\n",
    "accuracy = balanced_accuracy_score(labels_test_log, y_pred)\n",
    "print(\"Accuracy of Ridge Classifier: \", accuracy)\n",
    "auc = roc_auc_score(labels_test_log, y_pred)\n",
    "print(\"AUC score for Ridge Classifier: \", auc)\n",
    "print(\"F1 score for Ridge Classfier\", f1_score(labels_test_log, y_pred))\n",
    "\n",
    "#linearPCA.score(pca_400_test, labels_test)\n",
    "\n",
    "\n",
    "clf = LogisticRegression().fit(pca_400_train, labels_train_log)\n",
    "pred = clf.predict(pca_400_test)\n",
    "acc = balanced_accuracy_score(labels_test_log, pred)\n",
    "\n",
    "print(\"Accuracy of Logistic regression: \", acc)\n",
    "auc_log = roc_auc_score(labels_test_log, pred)\n",
    "print(\"AUC score for Logistic regression: \", auc_log)\n",
    "print(\"F1 score for Logistic regression\", f1_score(labels_test_log, pred))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#linearAE = RidgeClassifier().fit(outputs[29][1].cpu().numpy().flatten().reshape(-1, 1), labels)\n",
    "#linearAE.score(outputs[29][1].cpu().numpy(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d658c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "\n",
    "class ESN(BaseEstimator):\n",
    "    def __init__(self, input_size, reservoir_size, output_size, leaking_rate=0.95, spectral_radius=1, input_scaling=1, threshold=0.5, sparsity=0.1, ridge_alpha=0.5, random_seed=75):\n",
    "        np.random.seed(random_seed)\n",
    "        self.input_size = input_size\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.W_in = np.random.randn(reservoir_size, input_size)\n",
    "        self.W_res = np.random.randn(reservoir_size, reservoir_size)\n",
    "        #self.W_out = np.random.randn(output_size, reservoir_size)\n",
    "        \n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.input_scaling = input_scaling\n",
    "        self.threshold = threshold\n",
    "        self.sparsity = sparsity\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        \n",
    "        e, v = np.linalg.eig(self.W_res)\n",
    "        max_abs = np.max(np.abs(e))\n",
    "        self.W_res = self.W_res/max_abs\n",
    "        M = (np.random.uniform(size=(reservoir_size, reservoir_size)) < self.sparsity) * 1.0\n",
    "        self.W_res *= M\n",
    "        \n",
    "        self.h0 = np.zeros((reservoir_size, 1))\n",
    "        \n",
    "        #print(\"W_res: \", self.W_res.)\n",
    "        #print(max_abs)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def update(self, x):\n",
    "        h = self.h0\n",
    "        T = x.shape[0]\n",
    "        states = np.zeros((T, self.reservoir_size))\n",
    "\n",
    "        for t in range(T):\n",
    "            #print(x[t].reshape(-1, 1).shape)\n",
    "            h = self.leaking_rate * np.tanh(self.spectral_radius * self.W_res @ h + self.input_scaling * self.W_in @ \n",
    "                                                                                 x[t].reshape(-1, 1)) + (1-self.leaking_rate)*h\n",
    "            \n",
    "            states[t] = h.flatten()\n",
    "            \n",
    "        #print(states.shape)\n",
    "        \n",
    "        return states\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        V, F, D = X.shape\n",
    "        self.W_out = np.random.randn(self.output_size, self.reservoir_size)\n",
    "        states = np.zeros((V, F, self.reservoir_size))\n",
    "        \n",
    "        for v in range(V):\n",
    "            states[v] = self.update(X[v])\n",
    "        \n",
    "        states = states.reshape(V*F, self.reservoir_size)\n",
    "        \n",
    "            \n",
    "        \n",
    "        ridge = Ridge(alpha=self.ridge_alpha)\n",
    "        weights = compute_sample_weight(class_weight='balanced', y=y.flatten())\n",
    "        ridge.fit(states, y.flatten(), sample_weight=weights)\n",
    "        self.W_out = ridge.coef_\n",
    "            \n",
    "            #print(W_out.shape)\n",
    "            #self.W_out = W_out\n",
    "            \n",
    "        #print(self.W_out.shape)\n",
    "            \n",
    "        #return self.W_out\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        V, F, D = X.shape\n",
    "        pred = np.zeros((V, F, 1))\n",
    "        probs = np.zeros((V, F, 1))\n",
    "        \n",
    "        print(\"W_Res:\")\n",
    "        print(self.W_res * self.spectral_radius)\n",
    "        print(\"=======================================================\")\n",
    "        print(\"W_in\")\n",
    "        print(self.W_in * self.input_scaling)\n",
    "        for v in range(V):\n",
    "            states = self.update(X[v])\n",
    "            \n",
    "            \n",
    "            for f in range(F):\n",
    "                state = states[f]\n",
    "                output = np.dot(state.reshape(-1, 1).T, self.W_out.reshape(self.reservoir_size, -1))\n",
    "                #y_prob = sigmoid(output)\n",
    "                pred[v][f] = (output >= self.threshold).astype(int)\n",
    "                #print(pred[v][f].shape)\n",
    "                probs[v][f] =  output\n",
    "        \n",
    "        return pred, probs\n",
    "            \n",
    "            \n",
    "            #print(states.shape)\n",
    "            #y = states @ self.W_out[v].T\n",
    "            #print(y.shape)\n",
    "            #print(self.W_out.shape)\n",
    "            #print(\"W_out\", self.W_out[v].shape)\n",
    "            #print(\"states\", states.shape)\n",
    "            \n",
    "            #w = self.W_out[v].reshape(self.reservoir_size, 1)\n",
    "            #y_pred[v] = sigmoid(np.sum(w * states.reshape((1, F, self.reservoir_size, 1)), axis=2))\n",
    "\n",
    "            #print(\"New W: \", w.shape)\n",
    "            #w = self.W_out[v].reshape((1, F, self.reservoir_size, 1))\n",
    "            #s = states.reshape((1, F, self.reservoir_size, 1))\n",
    "            #print(\"w\", w.shape)\n",
    "            #print(\"s\", s.shape)\n",
    "            \n",
    "            #y_prob = np.zeros((F, 1))\n",
    "            #y_pred = np.dot(self.W_out, states.T)\n",
    "            #y_prob = softmax(y_pred)\n",
    "            #y_pred_labels = np.argmax(y_prob, axis=1)\n",
    "            #for t in range(F):\n",
    "            #    y_pred = np.dot(self.W_out, states[t])\n",
    "            #    y_prob[t] = sigmoid(y_pred)\n",
    "                \n",
    "            #y_prob = sigmoid(y_pred)\n",
    "            #print(y_prob.T.shape)\n",
    "            #probs[v] = y_prob.reshape(-1, 1)\n",
    "            #print(y_pred[2])\n",
    "            #print(\"y_pred\", y_pred.reshape(-1, 1).shape)\n",
    "            \n",
    "            #y_pred = softmax(self.W_out[v] @ states.T)\n",
    "            #print(y_pred.shape)\n",
    "            #pred[v] = (y_prob.reshape(-1, 1) >= self.threshold).astype(int)\n",
    "            \n",
    "            \n",
    "        #return pred, probs\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a332a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039fe04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1867b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "VF, D = pca_200_train.shape\n",
    "V = int(VF/200)\n",
    "F = 200\n",
    "train_data = pca_200_train.reshape(V, F, D)\n",
    "\n",
    "print(train_data.shape)\n",
    "\n",
    "#train_states = None\n",
    "\n",
    "    \n",
    "VF, D = pca_200_test.shape\n",
    "V = int(VF/200)\n",
    "F = 200\n",
    "test_data = pca_200_test.reshape(V, F, D)\n",
    "print(test_data.shape)\n",
    "\n",
    "\n",
    "X_train_new, X_val, y_train_new, y_val = train_test_split(train_data, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77727797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_hyp, X_val_hyp, y_train_hyp, y_val_hyp = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ee50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nf = 200\n",
    "H = 160\n",
    "W = 240\n",
    "batch_size = 50\n",
    "\n",
    "Nv_train = len(X_train_hyp)\n",
    "Nv_val = len(X_val_hyp)\n",
    "Nv_test = len(X_test)\n",
    "\n",
    "\n",
    "\n",
    "x_train_mean = X_train_hyp.mean(axis=0)\n",
    "x_train_nomean = X_train_hyp - x_train_mean\n",
    "x_val_nomean = X_val_hyp - x_train_mean\n",
    "x_test_nomean = X_test - x_train_mean\n",
    "\n",
    "\n",
    "x_train_nomean = x_train_nomean.reshape(Nv_train*Nf, 1, H, W)\n",
    "y_train_hyp = y_train_hyp.reshape(Nv_train*Nf, 1)\n",
    "\n",
    "\n",
    "x_val_nomean = x_val_nomean.reshape(Nv_val*Nf, 1, H, W)\n",
    "y_val_hyp = y_val_hyp.reshape(Nv_val*Nf, 1)\n",
    "\n",
    "print(x_train_nomean.shape)\n",
    "print(y_train_hyp.shape)\n",
    "\n",
    "train_Ntrue = y_train_hyp.sum()\n",
    "train_c1_w = 1.0/train_Ntrue\n",
    "train_c0_w = 1.0/(len(y_train_hyp) - train_Ntrue)\n",
    "train_c_wtot = train_c1_w + train_c0_w\n",
    "train_c1_w /= train_c_wtot\n",
    "train_c0_w /= train_c_wtot\n",
    "\n",
    "train_c_w = (1-y_train_hyp)*train_c0_w + y_train_hyp*train_c1_w\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(train_Ntrue, train_c0_w, train_c1_w)\n",
    "\n",
    "\n",
    "\n",
    "x_test_nomean = x_test_nomean.reshape(Nv_test*Nf, 1, H, W)\n",
    "y_test = y_test.reshape(Nv_test*Nf, 1)\n",
    "\n",
    "test_Ntrue = y_test.sum()\n",
    "test_c1_w = 1.0/test_Ntrue\n",
    "test_c0_w = 1.0/(len(y_test) - test_Ntrue)\n",
    "test_c_wtot = test_c0_w + test_c1_w\n",
    "test_c1_w /= test_c_wtot\n",
    "test_c0_w /= test_c_wtot\n",
    "\n",
    "test_c_w = (1-y_test)*test_c0_w + y_test*test_c1_w\n",
    "\n",
    "\n",
    "\n",
    "val_Ntrue = y_val_hyp.sum()\n",
    "val_c1_w = 1.0/val_Ntrue\n",
    "val_c0_w = 1.0/(len(y_val_hyp) - val_Ntrue)\n",
    "val_c_wtot = val_c1_w + val_c0_w\n",
    "val_c1_w /= val_c_wtot\n",
    "val_c0_w /= val_c_wtot\n",
    "\n",
    "val_c_w = (1-y_val_hyp)*val_c0_w + y_val_hyp*val_c1_w\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a4ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_expanded = np.repeat(x_train_nomean, 3, axis=1)\n",
    "print(x_train_expanded.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TensorDataset(torch.Tensor(x_train_nomean).to(device), torch.Tensor(y_train_hyp).to(device), torch.Tensor(train_c_w).to(device))\n",
    "train_dl = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#testset = TensorDataset(torch.Tensor(x_test_nomean).to(device), torch.Tensor(y_test).to(device), torch.Tensor(test_c_w).to(device))\n",
    "#test_dl = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#valset = TensorDataset(torch.Tensor(x_val_nomean).to(device), torch.Tensor(y_val_hyp).to(device), torch.Tensor(val_c_w).to(device))\n",
    "#val_dl = DataLoader(valset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421a819",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, hidden_size_1=1000, hidden_size_2=200):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=11, stride=4, padding=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=5, stride=4, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.fc1 = nn.Linear(16*10*15, hidden_size_1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        hidden = self.relu3(x)\n",
    "        x = self.fc2(hidden)\n",
    "        x = self.relu4(x)\n",
    "        return hidden, x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hidden_size_1 = 1000\n",
    "hidden_size_2 = 200\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, 11, padding=5, stride=4),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, 5, padding=2, stride=4),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(16*10*15, 1000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 40 * 60, 1000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200,1),\n",
    "    nn.Sigmoid()\n",
    ")\"\"\"\n",
    "\n",
    "\"\"\"model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 40 * 60, 1000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 400),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(400,1),\n",
    "    nn.Sigmoid()\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128 * 20 * 30, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#hidden_size_1 = 1000\n",
    "#hidden_size_2 = 200\n",
    "\"\"\"model = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(240*160, hidden_size_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size_1, hidden_size_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size_2, 1),\n",
    "            nn.Sigmoid()\n",
    ")\"\"\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "lr = 1e-6\n",
    "weight_decay = 1e-3\n",
    "\n",
    "#loss_criterion = nn.MSELoss() \n",
    "#loss_criterion = nn.BCELoss() \n",
    "#loss_criterion = lambda pred, true, weight: weight*\n",
    "loss_criterion = F.binary_cross_entropy\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "Nepochs=30\n",
    "\n",
    "\n",
    "\n",
    "tr_preds = np.zeros((len(train_dl), batch_size, 1))\n",
    "tr_trues = np.zeros((len(train_dl), batch_size, 1))\n",
    "\n",
    "#te_preds = np.zeros((len(test_dl), batch_size, 1))\n",
    "#te_trues = np.zeros((len(test_dl), batch_size, 1))\n",
    "\n",
    "\n",
    "\n",
    "for n in range(Nepochs):\n",
    "    model.train()\n",
    "    tr_avg_loss = 0.0\n",
    "    tr_avg_acc = 0.0\n",
    "    \n",
    "    for i,batch in enumerate(train_dl):\n",
    "        x, y_true, c_w = batch\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        c_w = c_w.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        y = model(x)\n",
    "        loss = loss_criterion(y, y_true, weight=c_w)\n",
    "        pred_class = (y > 0.5)\n",
    "        true_class = (y_true > 0.5)\n",
    "        tr_avg_acc += (pred_class == true_class).sum().item()\n",
    "\n",
    "        tr_preds[i] = pred_class.detach().cpu().numpy()\n",
    "        tr_trues[i] = true_class.detach().cpu().numpy()\n",
    "\n",
    "        tr_avg_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    tr_avg_loss /= len(train_dl)\n",
    "    tr_avg_acc /= len(train_dl)*batch_size\n",
    "\n",
    "    tr_prec = precision_score(tr_trues.flatten(), tr_preds.flatten(), zero_division=0)\n",
    "    tr_recall = recall_score(tr_trues.flatten(), tr_preds.flatten(), zero_division=0)\n",
    "    tr_f1 = f1_score(tr_trues.flatten(), tr_preds.flatten(), zero_division=0)\n",
    "    tr_bal_acc = balanced_accuracy_score(tr_trues.flatten(), tr_preds.flatten())\n",
    "\n",
    "    model.eval()\n",
    "    te_avg_loss = 0.0\n",
    "    te_avg_acc = 0.0\n",
    "\n",
    "\n",
    "    \"\"\"with torch.no_grad():\n",
    "        for i, batch in enumerate(test_dl):\n",
    "            x, y_true, c_w = batch\n",
    "            y = model(x)\n",
    "            loss = loss_criterion(y, y_true, weight=c_w)\n",
    "            te_avg_loss += loss.item()\n",
    "            pred_class = (y > 0.5)\n",
    "            true_class = (y_true > 0.5)\n",
    "            te_avg_acc += (pred_class == true_class).sum().item()\n",
    "            te_preds[i] = pred_class.detach().cpu().numpy()\n",
    "            te_trues[i] = true_class.detach().cpu().numpy()\n",
    "    te_avg_loss /= len(test_dl)\n",
    "    te_avg_acc /= len(test_dl)*batch_size\n",
    "    \n",
    "    te_prec = precision_score(te_trues.flatten(), te_preds.flatten(), zero_division=0)\n",
    "    te_recall = recall_score(te_trues.flatten(), te_preds.flatten(), zero_division=0)\n",
    "    te_f1 = f1_score(te_trues.flatten(), te_preds.flatten(), zero_division=0)\n",
    "    te_bal_acc = balanced_accuracy_score(te_trues.flatten(), te_preds.flatten())\"\"\"\n",
    "\n",
    "    #print(n, tr_avg_loss, tr_avg_acc, tr_bal_acc, te_avg_loss, te_avg_acc, tr_f1, te_f1)\n",
    "    #print(\"Epoch, Loss, Acc, Bal Acc, F1\")\n",
    "    #print(n, tr_avg_loss, tr_avg_acc, tr_bal_acc, tr_f1)\n",
    "    print(\"Epoch: {}, Train Loss: {:.4f}, Acc: {:.4f}, Bal Acc: {:.4f}, F1: {:.4f}\".format(n, tr_avg_loss, tr_avg_acc, tr_bal_acc, tr_f1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735573de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d1688",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_extractor = nn.Sequential(*list(model.children())[:-2])\n",
    "num_features = 512\n",
    "\n",
    "def extract_features(dataloader):\n",
    "    dl_size = len(dataloader)\n",
    "    print(dl_size)\n",
    "    print(batch_size)\n",
    "    \n",
    "    dl_size = dl_size*batch_size\n",
    "    features = np.zeros((dl_size, num_features))\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y_true, c_w = batch\n",
    "        #print(x.shape)\n",
    "        #y = pretrained_model(x)\n",
    "        #print(y.shape)\n",
    "\n",
    "        batch_features = feature_extractor(x)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = np.squeeze(batch_features.cpu().detach().numpy())\n",
    "        \n",
    "    print(features.shape)\n",
    "\n",
    "    features_reshaped = features.reshape(int(dl_size/200), 200, num_features)\n",
    "    return features_reshaped\n",
    "\n",
    "\n",
    "#trainset = TensorDataset(torch.Tensor(x_train_nomean).to(device), torch.Tensor(y_train_hyp).to(device), torch.Tensor(train_c_w).to(device))\n",
    "train_dl = DataLoader(trainset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "esn_input_train = extract_features(train_dl)\n",
    "esn_input_test = extract_features(test_dl)\n",
    "esn_input_val = extract_features(val_dl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4688cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(esn_input_train.shape)\n",
    "print(esn_input_train)\n",
    "\n",
    "y_train = y_train_hyp.reshape(42, 200, 1)\n",
    "y_test = y_test.reshape(14, 200, 1)\n",
    "y_val = y_val_hyp.reshape(14, 200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ff320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data_transforms = transforms.Compose([transforms.Grayscale(num_output_channels=3), transforms.ToTensor()])\n",
    "\n",
    "\n",
    "pretrained_model = models.resnet50(pretrained=True)\n",
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()\n",
    "\n",
    "# Remove the last layer (classification layer) to extract features\n",
    "feature_extractor = nn.Sequential(*list(pretrained_model.children())[:-1])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Define your grayscale image tensor\n",
    "\n",
    "\n",
    "# Forward pass to extract features\n",
    "features = feature_extractor(torch.Tensor(x_train_expanded).to(device))\n",
    "\n",
    "# Print the shape of the extracted features\n",
    "print(features.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a27624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vids_frames, dim = esn_input_train.shape\n",
    "#vids = int(vids_frames/200)\n",
    "#frames = 200\n",
    "#data = esn_input_train.reshape(vids, frames, dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_opt, X_val, y_train_opt, y_val = train_test_split(esn_input_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba5b3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "input_size = 512\n",
    "reservoir_size = 1000\n",
    "output_size = 1\n",
    "\n",
    "\n",
    "#np.random.seed(75)\n",
    "\n",
    "\n",
    "#def prune(trial):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    #reservoir_size = trial.suggest_int(\"reservoir_size\", 500, 3000)\n",
    "    spectral_radius = trial.suggest_float(\"spectral_radius\", 0.0, 1.0)\n",
    "    sparsity = trial.suggest_float(\"sparsity\", 0.0, 0.2)\n",
    "    leaking_rate = trial.suggest_float(\"leaking_rate\", 0.7, 0.99)\n",
    "    input_scaling = trial.suggest_float(\"input_scaling\", 0.0, 1.0)\n",
    "    threshold = trial.suggest_float(\"threshold\", -0.75, 1.75)\n",
    "    ridge_alpha = trial.suggest_float(\"ridge_alpha\", 0.00001, 1.0)\n",
    "    #random_seed = trial.suggest_int(\"random_seed\", 1, 200)\n",
    "\n",
    "    # Initialize the ESN with the given hyperparameters\n",
    "    esn = ESN(input_size=input_size, reservoir_size=reservoir_size, output_size=output_size, \n",
    "              leaking_rate=leaking_rate, spectral_radius=spectral_radius, input_scaling=input_scaling, \n",
    "              threshold=threshold, sparsity=sparsity, ridge_alpha=ridge_alpha, random_seed=83)\n",
    "    \n",
    "    \n",
    "    #esn = ESN(input_size=input_size, reservoir_size=reservoir_size, output_size=output_size, \n",
    "    #          leaking_rate=0.95, spectral_radius=spectral_radius, input_scaling=input_scaling, sparsity=0.1, \n",
    "    #          ridge_alpha=0.5, threshold=threshold, random_seed=83)\n",
    "    \n",
    "    #esn = ESN(input_size=input_size, reservoir_size=reservoir_size, output_size=output_size,\n",
    "    #         leaking_rate=0.95, sparsity=0.1, random_seed=random_seed)\n",
    "\n",
    "    # Train and evaluate the ESN on the training set\n",
    "    #esn.fit(X_train_opt, y_train_opt)\n",
    "    #esn.fit(X_train_new, y_train_new)\n",
    "    esn.fit(esn_input_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_pred, y_prob = esn.predict(esn_input_val)\n",
    "    score = f1_score(y_val.flatten(), y_pred.flatten())\n",
    "    #score = roc_auc_score(y_val.flatten(), y_prob.flatten())\n",
    "    print(\"Balanced acc: \", balanced_accuracy_score(y_val.flatten(), y_pred.flatten()))\n",
    "    print(\"F1: \", f1_score(y_val.flatten(), y_pred.flatten()))\n",
    "    print(\"AUC: \", roc_auc_score(y_val.flatten(), y_prob.flatten()))\n",
    "    \n",
    "    y_prob = y_prob.flatten()\n",
    "    anomaly_probs = y_prob[y_val.flatten().astype(bool)]\n",
    "    non_anomaly_probs = y_prob[~y_val.flatten().astype(bool)]\n",
    "\n",
    "    plt.hist(anomaly_probs, bins=50, alpha=0.5, label='Anomaly')\n",
    "    plt.hist(non_anomaly_probs, bins=50, alpha=0.5, label='Normal')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    #trial_number = trial.number\n",
    "    #if prune(trial):\n",
    "    #    raise optuna.exceptions.TrialPruned()\n",
    "    #print(trial.params, score)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "\n",
    "study.optimize(objective, n_trials=100)#, n_jobs=-1)\n",
    "\n",
    "\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best score: {best_score}\")\n",
    "optuna.visualization.plot_param_importances(study)\n",
    "plot_optimization_history(study)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514790df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077d1b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#esn = ESN(input_size=400, reservoir_size=500, output_size=1, alpha=0.5, gamma=0.1, spectral_radius=0.95, sparsity=0.1)\n",
    "#esn = ESN(input_scaling=0.6075448519014384, input_size=400,\n",
    "    #leaking_rate=0.17052412368729153, output_size=1, reservoir_size=1000,\n",
    "    #sparsity=0.06505159298527952, spectral_radius=0.9488855372533332,\n",
    "    #threshold=0.9656320330745594)\n",
    "    \n",
    "#np.random.seed(42)\n",
    "#esn = ESN(input_scaling=0.6998812876189486, input_size=400,\n",
    "#    leaking_rate=0.08210799041387684, output_size=1, reservoir_size=1000,\n",
    "#    sparsity=0.9252733800015754, spectral_radius=0.3364410948180354, threshold=0.39462707778742845)\n",
    "\n",
    "#0.9219100448194322\n",
    "\n",
    "\n",
    "esn = ESN(input_size=512, reservoir_size=1000, output_size=1, leaking_rate=0.804889704825926, spectral_radius=0.11146548076698023, \n",
    "          sparsity=0.02233328574171222, input_scaling=0.7663835438773781, threshold=0.15999184432185015, ridge_alpha=0.5101612068080433, random_seed=83)\n",
    "\n",
    "\n",
    "\n",
    "#train_ESN(esn, pca_400, labels, epochs)\n",
    "\n",
    "\n",
    "# ESN trained using dataset as shape (V*F, D)\n",
    "#train_states = esn.fit(pca_400_train, labels_train)\n",
    "#y_pred = esn.predict(pca_400_test)\n",
    "#esn_accuracy = recall_score(labels_test, y_pred)\n",
    "#print(\"ESN Accuracy: \", esn_accuracy)\n",
    "#auc_score = roc_auc_score(labels_test, y_pred)\n",
    "#print(\"ESN AUC: \", auc_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Trained using (V, F, D)\n",
    "#VF, D = pca_400_train.shape\n",
    "#V = int(VF/200)\n",
    "#F = 200\n",
    "#train_data = pca_400_train.reshape(V, F, D)\n",
    "\n",
    "#train_states = None\n",
    "\n",
    "\n",
    "\n",
    "#esn.fit(esn_input_train, y_train)\n",
    "\n",
    "esn.fit(esn_input_train, y_train)\n",
    "    \n",
    "#VF, D = pca_400_test.shape\n",
    "#V = int(VF/200)\n",
    "#F = 200\n",
    "#test_data = pca_400_test.reshape(V, F, D)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#acc = np.zeros((V))\n",
    "#for i in range(V):\n",
    "#    y_pred = esn.predict(test_data[i])\n",
    "#    esn_accuracy = recall_score(y_test[i], y_pred)\n",
    "#    acc[i] = esn_accuracy\n",
    "#    if 1 in y_test[i]:\n",
    "#        auc_score = roc_auc_score(y_test[i], y_pred)\n",
    "#        print(f\"Accuracy for video {i}: {esn_accuracy}     AUC for video {i}: {auc_score}\")\n",
    "#    else:\n",
    "#        print(f\"Accuracy for video {i}: {esn_accuracy}\")\n",
    "\n",
    "\n",
    "#print(np.mean(acc))\n",
    "\n",
    "\n",
    "\n",
    "#esn_pred, prob = esn.predict(esn_input_test)\n",
    "esn_pred, prob = esn.predict(esn_input_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test.flatten(), prob.flatten())\n",
    "auc_score = roc_auc_score(y_test.flatten(), prob.flatten())\n",
    "\n",
    "\n",
    "print(\"Actual anomalies: \", np.count_nonzero(y_test.flatten()))\n",
    "print(\"Predicted anomalies\", np.count_nonzero(esn_pred.flatten()))\n",
    "print(prob.flatten())\n",
    "print(\"Pred\", esn_pred.shape)\n",
    "print(\"Pred flattened\", esn_pred.flatten().shape)\n",
    "print(\"y_test\", y_test.shape)\n",
    "print(\"y_test flattened\", y_test.flatten().shape)\n",
    "#print((prob >= 0.5).astype(int))\n",
    "\n",
    "#print(esn_pred.flatten())\n",
    "#print(y_test.flatten())\n",
    "balanced_accuracy = balanced_accuracy_score(y_test.flatten(), esn_pred.flatten())\n",
    "print(\"Bal Acc: \", balanced_accuracy)\n",
    "print(\"F1: \", f1_score(y_test.flatten(), esn_pred.flatten()))\n",
    "print(\"AUC: \", auc_score)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title(f'ROC Curve (AUC = {auc_score:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test.flatten(), esn_pred.flatten())\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#esn_accuracy = accuracy_score(y_test, y_pred)\n",
    "#print(\"ESN Accuracy: \", esn_accuracy)\n",
    "#auc_score = roc_auc_score(y_test, y_pred)\n",
    "#print(\"ESN AUC: \", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af8d2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(prob.flatten())), prob.flatten())\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Predicted probability')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(prob.flatten(), bins=10)\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "anomalies = y_test.astype(bool)\n",
    "non_anomalies = ~anomalies\n",
    "\n",
    "# plot scatter plot\n",
    "plt.scatter(np.where(anomalies)[0], prob[anomalies], color='red', label='anomalies')\n",
    "plt.scatter(np.where(non_anomalies)[0], prob[non_anomalies], color='blue', label='non-anomalies')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Predicted Probability')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_prob = prob.flatten()\n",
    "anomaly_probs = y_prob[y_val.flatten().astype(bool)]\n",
    "non_anomaly_probs = y_prob[~y_val.flatten().astype(bool)]\n",
    "\n",
    "plt.hist(anomaly_probs, bins=50, alpha=0.5, label='Anomaly')\n",
    "plt.hist(non_anomaly_probs, bins=50, alpha=0.5, label='Normal')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
